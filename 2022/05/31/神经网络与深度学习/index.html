<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|微软雅黑:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="为了环境项目而学习的一些东西。">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络与深度学习">
<meta property="og:url" content="http://example.com/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="渔塘">
<meta property="og:description" content="为了环境项目而学习的一些东西。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E5%99%A8%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B.png">
<meta property="og:image" content="http://example.com/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg">
<meta property="og:image" content="http://example.com/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8F%8C%E8%BE%93%E5%85%A5%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C.jpg">
<meta property="og:image" content="http://example.com/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BB%80%E4%B9%88%E6%98%AF%E5%BC%A0%E9%87%8F.jpg">
<meta property="article:published_time" content="2022-05-31T13:21:47.000Z">
<meta property="article:modified_time" content="2025-01-02T13:03:26.465Z">
<meta property="article:author" content="CYanoii">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E5%99%A8%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B.png">

<link rel="canonical" href="http://example.com/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>神经网络与深度学习 | 渔塘</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">渔塘</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">人生苦短，摸鱼乃大</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-链接">

    <a href="/links" rel="section"><i class="fas fa-link fa-fw"></i>链接</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="CYanoii">
      <meta itemprop="description" content="✨ 退役 ACMer / 搞点代码 ✨">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="渔塘">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          神经网络与深度学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-31 21:21:47" itemprop="dateCreated datePublished" datetime="2022-05-31T21:21:47+08:00">2022-05-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-01-02 21:03:26" itemprop="dateModified" datetime="2025-01-02T21:03:26+08:00">2025-01-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>
            <div class="post-description">为了环境项目而学习的一些东西。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>机器学习包括神经网络，神经网络是深度学习的重要前置知识点。</p>
<p>神经网络是一种模拟人脑神经元的方法。</p>
<p>经典神经网络结构图包括输入层、隐藏层、输出层。中间有单向的连线，每个连线对应一个不同的<strong>权重</strong>，相当于人工神经网络的<strong>记忆</strong>，这是需要<strong>训练</strong>得到的。</p>
<h2 id="基本特征"><a href="#基本特征" class="headerlink" title="基本特征"></a>基本特征</h2><ol>
<li>非线性：不按比例，不成直线的关系。</li>
<li>非局限性：一个神经网络通常由多个神经元广泛连接而成。</li>
</ol>
<p>联想记忆是一个典型例子。</p>
<ol start="3">
<li>非常定性</li>
</ol>
<p>人工神经网络具有自适应、自组织、自学习能力。神经网络不但处理的信息可以有各种变化，而且在处理信息的同时，非线性动力系统本身也在不断变化。经常采用迭代过程描写动力系统的演化过程。</p>
<p>人工神经网络像一个黑盒子，用于模拟任何函数。由于其采用整体逼近的方式，不会由于个别样本误差而影响整个模型特性，即容错特性。</p>
<ol start="4">
<li>非凸性</li>
</ol>
<p>指这种函数有多个极值，故系统具有多个较稳定的平稳态，这将导致系统演化的多样性。</p>
<h1 id="人工神经元MP模型与-Hebb-学习律"><a href="#人工神经元MP模型与-Hebb-学习律" class="headerlink" title="人工神经元MP模型与 Hebb 学习律"></a>人工神经元MP模型与 Hebb 学习律</h1><p>这里需要一些图。</p>
<p>当时的权值都是固定的，预先设置好的。</p>
<h2 id="Hebb-学习律"><a href="#Hebb-学习律" class="headerlink" title="Hebb 学习律"></a>Hebb 学习律</h2><p>Hebb 考虑调整权值来机器学习。</p>
<p>反复刺激会加强刺激效果。</p>
<h3 id="线性联想器"><a href="#线性联想器" class="headerlink" title="线性联想器"></a>线性联想器</h3><p>这里需要一些图。</p>
<p>用目标输出代替实际输出。</p>
<p>$$<br>w_{ij} &#x3D; w_{ij} + t_i p_j<br>$$</p>
<p>$$<br>W &#x3D; TP^T<br>$$</p>
<p>带入</p>
<p>$$<br>Wp_1, Wp_2<br>$$</p>
<h1 id="感知器网络"><a href="#感知器网络" class="headerlink" title="感知器网络"></a>感知器网络</h1><p>监督学习：使用已标记的数据集训练</p>
<p>分类、回归</p>
<p>半监督学习：使用一部分标记数据和大量未标记数据做数据集训练</p>
<p>分类、回归、聚类</p>
<p>无监督学习：使用未标记的数据集训练</p>
<p>聚类</p>
<ul>
<li>感知机由美国计算机科学家罗森布拉特于1957年提出，其神经元结构就是 MP 模型。</li>
<li>感知机神经元是在 MP 模基础上加上了学习功能，其权值根据设计目的加以调节。</li>
<li>感知机特别适合简单的模式分类问题，可解决线性划分问题。</li>
</ul>
<img src="/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E5%99%A8%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B.png" class="" title="img">

<p>$$<br>n &#x3D; \sum^r_{j &#x3D; 1} w_j p_j + b	\quad a &#x3D; f(n) &#x3D;<br>\begin{cases}<br>1 \quad n \geq 0 \\<br>0 \quad n &lt; 0 \\<br>\end{cases}<br>$$</p>
<p>$$<br>a &#x3D; f(W P + b) &#x3D; f(\sum^r_{j &#x3D; 1} w_j p_j + b) &#x3D;<br>\begin{cases}<br>1 \quad n \geq 0 \\<br>0 \quad n &lt; 0 \\<br>\end{cases}<br>$$</p>
<ul>
<li>一般感知机结构是一单层神经网络，其激活函数为二值函数。</li>
</ul>
<img src="/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%84%9F%E7%9F%A5%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg" class="" title="img">

<p><strong>感知器的权值与偏差</strong></p>
<p>设置偏差 b 是为了增加神经网络可塑性，多一个可调参数。</p>
<p>$$<br>W_{s \cdot r} &#x3D;<br>\begin{bmatrix}<br>w_{11} &amp; w_{12} &amp; \cdots &amp; w_{1r} \\<br>w_{21} &amp; w_{22} &amp; \cdots &amp; w_{2r} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>w_{s1} &amp; w_{s2} &amp; \cdots &amp; w_{sr} \\<br>\end{bmatrix}<br>,<br>B_{s \cdot 1} &#x3D;<br>\begin{bmatrix}<br>b_1 \\<br>b_2 \\<br>\vdots \\<br>b_s \\<br>\end{bmatrix}<br>$$</p>
<p><strong>感知器的多组输入输出</strong></p>
<p>$$<br>P_{r \cdot q} &#x3D;<br>\begin{bmatrix}<br>p_{11} &amp; p_{12} &amp; \cdots &amp; p_{1q} \\<br>p_{21} &amp; p_{22} &amp; \cdots &amp; p_{2q} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>p_{r1} &amp; p_{r2} &amp; \cdots &amp; p_{rq} \\<br>\end{bmatrix}<br>,<br>A_{r \cdot q} &#x3D;<br>\begin{bmatrix}<br>a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1q} \\<br>a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2q} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>a_{s1} &amp; a_{s2} &amp; \cdots &amp; a_{sq} \\<br>\end{bmatrix}<br>,<br>B_{s \cdot q} &#x3D;<br>\begin{bmatrix}<br>B_{s \cdot 1} &amp; B_{s \cdot 1} &amp; \cdots &amp; B_{s \cdot 1}<br>\end{bmatrix}<br>$$</p>
<p><strong>感知器的图形解释</strong></p>
<p>任意一组参数 W 和 b ，在输入矢量空间中，可决定一条（超）直线或（超）平面等，在该直线或平面上方输出为 1 ，下放输出为 0 。</p>
<p><strong>感知器的输入输出关系</strong></p>
<p>分类能力：感知器只能进行线性分类。</p>
<p>分类数目：单个神经元可将输入空间的矢量非储层两类。</p>
<p>s 个神经元组成的感知器将输入空间分成$2^s$类。</p>
<p>我们将构成 W 的第 i 个行向量定义为：</p>
<p>$$<br>i^W &#x3D;<br>\begin{bmatrix}<br>w_{i, 1} \\<br>w_{i, 2} \\<br>\vdots \\<br>w_{i, R} \\<br>\end{bmatrix}<br>$$</p>
<p>据此，可将权值矩阵 W 重写为：</p>
<p>$$<br>W &#x3D;<br>\begin{bmatrix}<br>{1^W}^T \\<br>{2^W}^T \\<br>\vdots \\<br>{S^W}^T \\<br>\end{bmatrix}<br>$$</p>
<p>这样就可以将网络输出向量的第 i 个元素写成：</p>
<p>$$<br>a_i &#x3D; hardlim(n_i) &#x3D; hardlim({i^W}^T p + b_i)<br>$$</p>
<h2 id="单神经元感知器"><a href="#单神经元感知器" class="headerlink" title="单神经元感知器"></a>单神经元感知器</h2><p>s &#x3D; 1, r &#x3D; 2。</p>
<img src="/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8F%8C%E8%BE%93%E5%85%A5%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C.jpg" class="" title="img">

<p>边界上所有点输入向量与权值向量的内积相同。因此该边界一定与权值向量垂直。</p>
<p>阴影区内任意输入向量都有着大于 -b 的内积，无阴影区内任意输入向量都有着小于 -b 的内积。</p>
<p>因此权值向量$1^W$总是指向神经元输出为 1 的区域。</p>
<h3 id="与门逻辑感知器"><a href="#与门逻辑感知器" class="headerlink" title="与门逻辑感知器"></a>与门逻辑感知器</h3><p>图</p>
<h3 id="或门逻辑感知器"><a href="#或门逻辑感知器" class="headerlink" title="或门逻辑感知器"></a>或门逻辑感知器</h3><p>图</p>
<h3 id="线性不可分问题"><a href="#线性不可分问题" class="headerlink" title="线性不可分问题"></a>线性不可分问题</h3><p>异或问题</p>
<p>不能解决</p>
<h2 id="感知器的学习规则"><a href="#感知器的学习规则" class="headerlink" title="感知器的学习规则"></a>感知器的学习规则</h2><ul>
<li>如果第 i 个神经元输出正确，即$a_i &#x3D; t_i$，权值与偏差保持不变。</li>
<li>如果第 i 个神经元输出不正确，改变权值与偏差。<ul>
<li>若$a_i &#x3D; 0, t_i &#x3D; 1$，则修正的算法为：$w_{new} &#x3D; w_{old} + p^T, b_i &#x3D; b_i + 1$。</li>
<li>若$a_i &#x3D; 1, t_i &#x3D; 0$，则修正的算法为：$w_{new} &#x3D; w_{old} - p^T, b_i &#x3D; b_i - 1$。</li>
</ul>
</li>
</ul>
<p>一点说明：在$a_i &#x3D; 1, t_i &#x3D; 0$的情况中，$旧 n_i &#x3D; W_i P + b \geq 0$，但$新 n_i &#x3D; (W_i - p^T) P + b &#x3D; 旧 n_i - P^T P &lt; 旧 n_i$。</p>
<p><strong>学习的统一表达式：</strong></p>
<p>感知器修正权值公式：</p>
<ul>
<li>分量表示：</li>
</ul>
<p>$$<br>\begin{aligned}<br>&amp; \Delta  w_{ij} &#x3D; (t_i - a_i) \cdot p_j^T \\<br>&amp; \Delta  b_i &#x3D; (t_i - a_i) \cdot 1 \\<br>\end{aligned}<br>$$</p>
<ul>
<li>矩阵表示：</li>
</ul>
<p>设$E &#x3D; T - A$为误差矢量，</p>
<p>$$<br>\begin{aligned}<br>&amp; W &#x3D; W + E P^T \quad or \quad \Delta W &#x3D; E P^T \\<br>&amp; B &#x3D; B + E \quad or \quad \Delta B &#x3D; E \\<br>\end{aligned}<br>$$</p>
<p><strong>学习的收敛性：</strong>该算法属于梯度下降法，有解时收敛。</p>
<h1 id="BP-神经网络"><a href="#BP-神经网络" class="headerlink" title="BP 神经网络"></a>BP 神经网络</h1><ul>
<li>单层感知器只能进行线性可分的运算（如不能解决异或问题）</li>
<li>多层感知器可解决线性不可分问题，双隐层感知器就足以解决任何复杂的分类问题<ul>
<li>隐层的权值怎么训练</li>
<li>隐层节点不存在期望输出</li>
<li>无法通过感知器的学习规则来训练多层感知器</li>
</ul>
</li>
</ul>
<p>多层前馈网络</p>
<p>单隐层网络</p>
<p>多隐层网络</p>
<p>层与层之间为全互连结构</p>
<p>&#x3D;&#x3D;分割线&#x3D;&#x3D;</p>
<ul>
<li><p>BP 网络的输入层和输出层节点个数确定</p>
<ul>
<li>输入个数根据特征的维度来定</li>
<li>输出个数根据要进行分类的类别数来定</li>
</ul>
</li>
<li><p>如何确定隐层节点的个数</p>
<ul>
<li>经验公式</li>
</ul>
<p>$$<br>h &#x3D; \sqrt {m + n} + a<br>$$</p>
<p>其中 h 为隐层节点的个数， m 为输入个数， n 为输出个数， a 为 1~10 之间的调节常数</p>
</li>
</ul>
<p>隐层神经元个数越多，分类效果越好，但计算量会增大</p>
<p>&#x3D;&#x3D;分割线&#x3D;&#x3D;</p>
<ul>
<li>由于线性模型的表达能力不够，考虑通过激活函数来加入非线性因素， BP 算法要求激活函数可导。</li>
<li>由于 S 形函数是可导的（导函数是连续函数），因此适合用在 BP 神经网络中。</li>
</ul>
<p>Sigmoid 函数</p>
<p>$$<br>f(x) &#x3D; {1 \over 1 + e^{-x}}<br>$$</p>
<h2 id="（信息）正向传播过程"><a href="#（信息）正向传播过程" class="headerlink" title="（信息）正向传播过程"></a>（信息）正向传播过程</h2><p>正向传播过程：输入数据从输入层传播到输出层的过程。</p>
<h2 id="（误差）反向传播过程"><a href="#（误差）反向传播过程" class="headerlink" title="（误差）反向传播过程"></a>（误差）反向传播过程</h2><p>从输出节点开始，反向地向第一隐含层传播由总误差引起的权值修正</p>
<ul>
<li>首先计算输出层单元地误差，并用该误差调整输出层地权值</li>
<li>根据输出层的误差计算隐层单元的误差，并用<strong>对应计算误差</strong>调整输出层的权值</li>
</ul>
<h2 id="BP-学习算法"><a href="#BP-学习算法" class="headerlink" title="BP 学习算法"></a>BP 学习算法</h2><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><ul>
<li>正向传播时，输入样本从输入层经各隐层逐层处理后，传向输出层。若输出层的实际输出与期望的输出不符，则转入<strong>误差</strong>的反向传播阶段。</li>
<li>反向传播时，将<strong>输出误差</strong>以<strong>某种形式</strong>通过隐层向输入层逐层反转，并将<strong>误差</strong>分摊给各层的所有单元，使各层单元获得<strong>误差</strong>信号作为修正各单元权值的依据。</li>
</ul>
<h3 id="误差与损失函数"><a href="#误差与损失函数" class="headerlink" title="误差与损失函数"></a>误差与损失函数</h3><p><strong>误差定义</strong></p>
<p>$$<br>E(w) &#x3D; {1 \over 2} \sum_{d \in D} (t_d - o_d)^2<br>$$</p>
<p>其中，$D$为训练样例集合，$t_d$为训练样例$d$的期望输出，$o_d$为训练样例$d$的实际输出。</p>
<p>以误差作为误差函数的经验风险，在不考虑正则项的情况下，就以误差函数作为损失函数。</p>
<h3 id="学习目的和学习方法"><a href="#学习目的和学习方法" class="headerlink" title="学习目的和学习方法"></a>学习目的和学习方法</h3><p><strong>学习目的</strong></p>
<p>最小化误差：$min E(w)$</p>
<p><strong>学习方法</strong></p>
<p>梯度下降</p>
<p>$$<br>\Delta w_{ij} &#x3D; - \eta {\partial E_d \over \partial w_{ij}}<br>$$</p>
<p>其中$E_d$为训练样本$d$的误差，$w_{ij}$与单元$j$的第$i$个输入相关连的权值</p>
<p>$$<br>w_{ij}(t + 1) &#x3D; w_{ij}(t) + \Delta w_{ij}<br>$$</p>
<h3 id="求导的链式法则"><a href="#求导的链式法则" class="headerlink" title="求导的链式法则"></a>求导的链式法则</h3><p>高数里的，不去记了，要将该法拓展到向量</p>
<p>后面的推导要记</p>
<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><h2 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h2><img src="/2022/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BB%80%E4%B9%88%E6%98%AF%E5%BC%A0%E9%87%8F.jpg" class="" title="img">

<p><strong>Pytorch0.4.0 以前</strong></p>
<p><code>Variable</code> 是 <code>torch.autograd</code> 中的数据类型，主要用于封装 Tensor ，进行<strong>自动求导</strong>。</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>data</code></td>
<td>被包装的 Tensor</td>
</tr>
<tr>
<td><code>grad</code></td>
<td>data 的梯度</td>
</tr>
<tr>
<td><code>grad_fn</code></td>
<td>创建 Tensor 的 Function ，是自动求导的关键</td>
</tr>
<tr>
<td><code>requires_grad</code></td>
<td>指示是否需要梯度</td>
</tr>
<tr>
<td><code>is_leaf</code></td>
<td>指示是否叶子结点（张量）</td>
</tr>
</tbody></table>
<p><strong>Pytorch0.4.0及以后</strong></p>
<p>单独有了 Tensor 类型。</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>dtype</code></td>
<td>张量的数据类型，有 9 种</td>
</tr>
<tr>
<td><code>shape</code></td>
<td>张量的形状</td>
</tr>
<tr>
<td><code>device</code></td>
<td>张量所在设备（GPU or CPU）</td>
</tr>
</tbody></table>
<p><code>dtpye</code> 常用 32 位浮点型（<code>torch.float32</code> or <code>torch.float</code>）或 64 位整型（<code>torch.int64</code> or <code>torch.lang</code>）。</p>
<h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><h4 id="直接创建"><a href="#直接创建" class="headerlink" title="直接创建"></a>直接创建</h4><p><strong><code>torch.tensor()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(data,       </span><br><span class="line">             dtype = <span class="literal">None</span>,</span><br><span class="line">             device = <span class="literal">None</span>,</span><br><span class="line">             requires_grad = <span class="literal">False</span>,</span><br><span class="line">             pin_memory = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>data</code></td>
<td>数据，可以是 list ， numpy</td>
</tr>
<tr>
<td><code>dtype</code></td>
<td>数据类型，默认与 <code>data</code> 的一致</td>
</tr>
<tr>
<td><code>device</code></td>
<td>所在设备， cuda&#x2F;cpu （cuda 就是 gpu）</td>
</tr>
<tr>
<td><code>requires_grad</code></td>
<td>是否需要梯度</td>
</tr>
<tr>
<td><code>pin_memory</code></td>
<td>是否存于锁页内存</td>
</tr>
</tbody></table>
<p><strong><code>torch.from_numpy()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.from_numpy(ndarray)</span><br></pre></td></tr></table></figure>

<p>功能：从 numpy 创建 tensor</p>
<p><strong>注意事项</strong>：从 <code>torch.from_numpy</code> 创建的 tensor 与原 ndarray <strong>共享内存</strong>，当修改其中一个的数据，另一个也将会被改动</p>
<h4 id="依据数值创建"><a href="#依据数值创建" class="headerlink" title="依据数值创建"></a>依据数值创建</h4><p><strong><code>torch.zeros()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*size,</span><br><span class="line">            out = <span class="literal">None</span>,</span><br><span class="line">            dtype = <span class="literal">None</span>,</span><br><span class="line">            layout = torch.strided,</span><br><span class="line">           	device = <span class="literal">None</span>,</span><br><span class="line">           	requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：依 <code>size</code> 创建全 0 张量</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>size</code></td>
<td>张量的形状，如 (3, 3) ， (3, 224, 224)</td>
</tr>
<tr>
<td><code>out</code></td>
<td>输出的张量</td>
</tr>
<tr>
<td><code>layout</code></td>
<td>内存中布局形式，有 <code>strided</code> ， <code>sparse_coo</code> 等</td>
</tr>
<tr>
<td><code>device</code></td>
<td>所在设备， gpu&#x2F;cpu</td>
</tr>
<tr>
<td><code>requires_grad</code></td>
<td>是否需要梯度</td>
</tr>
</tbody></table>
<p><strong><code>torch.zeros_like()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros_like(<span class="built_in">input</span>,</span><br><span class="line">                 dtype = <span class="literal">None</span>,</span><br><span class="line">                 layout = <span class="literal">None</span>,</span><br><span class="line">                 device = <span class="literal">None</span>,</span><br><span class="line">                 requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：依 <code>input</code> 形状创建全 0 张量</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>input</code></td>
<td>创建与 <code>input</code> 同形状的全 0 张量</td>
</tr>
<tr>
<td><code>dtpye</code></td>
<td>数据类型</td>
</tr>
<tr>
<td><code>layout</code></td>
<td>内存中布局形式</td>
</tr>
</tbody></table>
<p><strong><code>torch.ones()</code><strong>、</strong><code>torch.ones_like()</code><strong>、</strong><code>torch.full()</code><strong>、</strong><code>torch.full_like()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.ones()		<span class="comment"># 与上面同理，全1</span></span><br><span class="line">torch.ones_like()	<span class="comment"># 与上面同理，全1</span></span><br><span class="line"></span><br><span class="line">torch.full()		<span class="comment"># 与上面同理，在第1、2个参数间多一个参数fill_value，表示张量的值</span></span><br><span class="line">torch.full_like()	<span class="comment"># 与上面同理，在第1、2个参数间多一个参数fill_value，表示张量的值</span></span><br></pre></td></tr></table></figure>

<p><strong><code>torch.arrange()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.arrange(start = <span class="number">0</span>,</span><br><span class="line">              end,</span><br><span class="line">              step = <span class="number">1</span>,</span><br><span class="line">              out = <span class="literal">None</span>,</span><br><span class="line">              dtype = <span class="literal">None</span>,</span><br><span class="line">              layout = torch.strided,</span><br><span class="line">              device = <span class="literal">None</span>,</span><br><span class="line">              requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：创建等差的 1 维张量</p>
<p>注意事项：数值区间为 [start, end)</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>start</code></td>
<td>数列起始值</td>
</tr>
<tr>
<td><code>end</code></td>
<td>数列“结束值”</td>
</tr>
<tr>
<td><code>step</code></td>
<td>数列公差，默认为 1</td>
</tr>
</tbody></table>
<p><strong><code>torch.linspace()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.linspace(start,</span><br><span class="line">               end,</span><br><span class="line">               step = <span class="number">100</span>,</span><br><span class="line">               out = <span class="literal">None</span>,</span><br><span class="line">               dtype = <span class="literal">None</span>,</span><br><span class="line">               layout = torch.strided,</span><br><span class="line">               device = <span class="literal">None</span>,</span><br><span class="line">               requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：创建均分的 1 维张量</p>
<p>注意事项：数值区间维 [start, end]</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>start</code></td>
<td>数列起始值</td>
</tr>
<tr>
<td><code>end</code></td>
<td>数列结束值</td>
</tr>
<tr>
<td><code>steps</code></td>
<td>数列长度（数列有几个数）</td>
</tr>
</tbody></table>
<p><strong><code>torch.logspace()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.logspace(start,</span><br><span class="line">               end,</span><br><span class="line">               step = <span class="number">100</span>,</span><br><span class="line">               base = <span class="number">10.0</span>,</span><br><span class="line">               out = <span class="literal">None</span>,</span><br><span class="line">               dtype = <span class="literal">None</span>,</span><br><span class="line">               layout = torch.strided,</span><br><span class="line">               device = <span class="literal">None</span>,</span><br><span class="line">               requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：创建对数均分的 1 维张量</p>
<p>注意事项：长度为 <code>steps</code> ，底为 <code>base</code></p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>start</code></td>
<td>数列起始值</td>
</tr>
<tr>
<td><code>end</code></td>
<td>数列结束值</td>
</tr>
<tr>
<td><code>steps</code></td>
<td>数列长度（数列有几个数）</td>
</tr>
<tr>
<td><code>base</code></td>
<td>对数函数的底，默认为 10</td>
</tr>
</tbody></table>
<p><strong><code>torch.eye()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(n,</span><br><span class="line">          m = <span class="literal">None</span>,</span><br><span class="line">          out = <span class="literal">None</span>,</span><br><span class="line">          dtype = <span class="literal">None</span>,</span><br><span class="line">          layout = torch.strided,</span><br><span class="line">          device = <span class="literal">None</span>,</span><br><span class="line">          requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：创建单位对角矩阵（2 维张量）</p>
<p>注意事项：默认为方阵</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>n</code></td>
<td>矩阵行数</td>
</tr>
<tr>
<td><code>m</code></td>
<td>矩阵列数</td>
</tr>
</tbody></table>
<h4 id="依概率分布创建"><a href="#依概率分布创建" class="headerlink" title="依概率分布创建"></a>依概率分布创建</h4><p><strong><code>torch.normal()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean,</span><br><span class="line">             std,</span><br><span class="line">             out = <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">torch.normal(mean,</span><br><span class="line">             std,</span><br><span class="line">             size,</span><br><span class="line">             out = <span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：生成正态分布（高斯分布）</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>mean</code></td>
<td>均值</td>
</tr>
<tr>
<td><code>std</code></td>
<td>标准差</td>
</tr>
<tr>
<td><code>size</code></td>
<td>前二者都是<strong>标量</strong>时的创建个数</td>
</tr>
</tbody></table>
<p>四种模式：</p>
<table>
<thead>
<tr>
<th><code>mean</code></th>
<th><code>std</code></th>
</tr>
</thead>
<tbody><tr>
<td><strong>标量</strong></td>
<td><strong>标量</strong></td>
</tr>
<tr>
<td><strong>标量</strong></td>
<td>张量</td>
</tr>
<tr>
<td>张量</td>
<td><strong>标量</strong></td>
</tr>
<tr>
<td>张量</td>
<td>张量</td>
</tr>
</tbody></table>
<p><strong><code>torch.randn()</code><strong>、</strong><code>torch.randn_like()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(*size,</span><br><span class="line">            out = <span class="literal">None</span>,</span><br><span class="line">            dtype = <span class="literal">None</span>,</span><br><span class="line">            layout = torch.strided,</span><br><span class="line">            device = <span class="literal">None</span>,</span><br><span class="line">            requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：生成<strong>标准正态分布</strong></p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>size</code></td>
<td>张量的形状</td>
</tr>
</tbody></table>
<p><strong><code>torch.rand()</code><strong>、</strong><code>torch.rand_like()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.rand(*size,</span><br><span class="line">           out = <span class="literal">None</span>,</span><br><span class="line">           dtype = <span class="literal">None</span>,</span><br><span class="line">           layout = torch.strided,</span><br><span class="line">           device = <span class="literal">None</span>,</span><br><span class="line">           requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：在区间 [0, 1) 上，生成<strong>均匀分布</strong></p>
<p><strong><code>torch.randint()</code><strong>、</strong><code>torch.randint_like()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.randint(low = <span class="number">0</span>,</span><br><span class="line">              high,</span><br><span class="line">              size,</span><br><span class="line">              out = <span class="literal">None</span>,</span><br><span class="line">              dtype = <span class="literal">None</span>,</span><br><span class="line">              layout = torch.strided,</span><br><span class="line">              device = <span class="literal">None</span>,</span><br><span class="line">              requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：区间 [low, high) 生成整数<strong>均匀分布</strong></p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>size</code></td>
<td>张量的形状</td>
</tr>
</tbody></table>
<p><strong><code>torch.randperm()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.randprem(n,</span><br><span class="line">               out = <span class="literal">None</span>,</span><br><span class="line">               dtype = <span class="literal">None</span>,</span><br><span class="line">               layout = torch.strided,</span><br><span class="line">               device = <span class="literal">None</span>,</span><br><span class="line">               requires_grad = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：生成从 0 到 n - 1 的随机排列</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>n</code></td>
<td>张量的长度</td>
</tr>
</tbody></table>
<p><strong><code>torch.bernoulli()</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.bernoulli(<span class="built_in">input</span>,</span><br><span class="line">                *,</span><br><span class="line">                generator = <span class="literal">None</span>,</span><br><span class="line">                out = <span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：以 <code>input</code> 为概率，生成伯努利分布（0 - 1 分布，两点分布）</p>
<table>
<thead>
<tr>
<th>值</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>input</code></td>
<td>概率值</td>
</tr>
</tbody></table>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/05/24/%E3%80%90%E9%A2%98%E8%A7%A3%E3%80%91Educational-Codeforces-Round-129-Rated-for-Div-2-A-D/" rel="prev" title="【题解】Educational Codeforces Round 129 (Rated for Div. 2) A-D">
      <i class="fa fa-chevron-left"></i> 【题解】Educational Codeforces Round 129 (Rated for Div. 2) A-D
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/06/01/%E5%85%AD%E4%B8%80/" rel="next" title="六一">
      六一 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E7%89%B9%E5%BE%81"><span class="nav-number">1.1.</span> <span class="nav-text">基本特征</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E5%85%83MP%E6%A8%A1%E5%9E%8B%E4%B8%8E-Hebb-%E5%AD%A6%E4%B9%A0%E5%BE%8B"><span class="nav-number">2.</span> <span class="nav-text">人工神经元MP模型与 Hebb 学习律</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Hebb-%E5%AD%A6%E4%B9%A0%E5%BE%8B"><span class="nav-number">2.1.</span> <span class="nav-text">Hebb 学习律</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E8%81%94%E6%83%B3%E5%99%A8"><span class="nav-number">2.1.1.</span> <span class="nav-text">线性联想器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">感知器网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E7%A5%9E%E7%BB%8F%E5%85%83%E6%84%9F%E7%9F%A5%E5%99%A8"><span class="nav-number">3.1.</span> <span class="nav-text">单神经元感知器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8E%E9%97%A8%E9%80%BB%E8%BE%91%E6%84%9F%E7%9F%A5%E5%99%A8"><span class="nav-number">3.1.1.</span> <span class="nav-text">与门逻辑感知器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%88%96%E9%97%A8%E9%80%BB%E8%BE%91%E6%84%9F%E7%9F%A5%E5%99%A8"><span class="nav-number">3.1.2.</span> <span class="nav-text">或门逻辑感知器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86%E9%97%AE%E9%A2%98"><span class="nav-number">3.1.3.</span> <span class="nav-text">线性不可分问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99"><span class="nav-number">3.2.</span> <span class="nav-text">感知器的学习规则</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BP-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">BP 神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%EF%BC%88%E4%BF%A1%E6%81%AF%EF%BC%89%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="nav-number">4.1.</span> <span class="nav-text">（信息）正向传播过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%EF%BC%88%E8%AF%AF%E5%B7%AE%EF%BC%89%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="nav-number">4.2.</span> <span class="nav-text">（误差）反向传播过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BP-%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-number">4.3.</span> <span class="nav-text">BP 学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="nav-number">4.3.1.</span> <span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.2.</span> <span class="nav-text">误差与损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%9B%AE%E7%9A%84%E5%92%8C%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.3.</span> <span class="nav-text">学习目的和学习方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%82%E5%AF%BC%E7%9A%84%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="nav-number">4.3.4.</span> <span class="nav-text">求导的链式法则</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch"><span class="nav-number">5.</span> <span class="nav-text">Pytorch</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89"><span class="nav-number">5.1.</span> <span class="nav-text">张量（Tensor）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA"><span class="nav-number">5.1.1.</span> <span class="nav-text">创建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B4%E6%8E%A5%E5%88%9B%E5%BB%BA"><span class="nav-number">5.1.1.1.</span> <span class="nav-text">直接创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E6%8D%AE%E6%95%B0%E5%80%BC%E5%88%9B%E5%BB%BA"><span class="nav-number">5.1.1.2.</span> <span class="nav-text">依据数值创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%9D%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E5%88%9B%E5%BB%BA"><span class="nav-number">5.1.1.3.</span> <span class="nav-text">依概率分布创建</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="CYanoii"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">CYanoii</p>
  <div class="site-description" itemprop="description">✨ 退役 ACMer / 搞点代码 ✨</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">86</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CYanoii</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">180k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">5:01</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'CA3hrlrhv1qLMiD9zfDM2guF-gzGzoHsz',
      appKey     : '4S8s4bbqfYAleeoNqzFCbH4X',
      placeholder: "留下一条友善的评论",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '8' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
